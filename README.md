# IC-LLM: V100-optimiertes LLM-Deployment System

Eine umfassende LÃ¶sung fÃ¼r das Deployment von Large Language Models (LLMs) auf NVIDIA Tesla V100 GPUs in der HAW Hamburg Informatik Compute Cloud (ICC).

<div align="center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="LLM Deployment" width="600px">
</div>

## ğŸŒŸ Hauptfeatures

- **V100-optimierte Konfiguration** fÃ¼r NVIDIA Tesla V100 GPUs
- **Zwei Inference-Engines**: Text Generation Inference (TGI) und vLLM
- **Multi-GPU-UnterstÃ¼tzung** mit bis zu 4x V100 GPUs im Sharded-Modus
- **OpenAI-kompatible REST API** fÃ¼r einfache Integration
- **Benutzerfreundliche WebUI** fÃ¼r Chat-Interaktionen
- **UnterstÃ¼tzung fÃ¼r zahlreiche Modelle**: Mistral, Llama, Gemma, Phi, etc.
- **Speicheroptimierungen**: AWQ Quantisierung, optimierte KontextlÃ¤ngen

## ğŸ“‹ Voraussetzungen

- HAW Hamburg infw-Account mit ICC-Zugang
- kubectl-Client auf Ihrem lokalen System
- VPN-Verbindung zum HAW-Netz (bei Remote-Zugriff)

## ğŸš€ Schnellstart

```bash
# Repository klonen
git clone https://github.com/scimbe/icc-Inference-deployment.git
cd icc-Inference-deployment

# Berechtigung setzen
chmod +x scripts/*.sh
chmod +x *.sh

# ICC-Login durchfÃ¼hren (einmalig)
./scripts/icc-login.sh

# Deployment starten (interaktiver Modus)
./deploy-v100.sh
```

Nach dem Deployment kÃ¶nnen Sie die WebUI unter http://localhost:3000 und die API unter http://localhost:8000 erreichen.

## ğŸ–¥ï¸ Manuelle Installation

```bash
# 1. V100-optimierte Konfiguration kopieren
cp configs/config.v100.sh configs/config.sh

# 2. Konfiguration anpassen (wichtig!)
#    - NAMESPACE auf Ihre w-Kennung + "-default" setzen
#    - Modell und GPU-Anzahl wÃ¤hlen
nano configs/config.sh

# 3. TGI mit V100-Optimierungen deployen
./scripts/deploy-tgi-v100.sh

# 4. ODER: vLLM deployen
./scripts/deploy-vllm-v100.sh

# 5. Web-OberflÃ¤che installieren
./scripts/deploy-webui.sh

# 6. Zugriff einrichten
./scripts/port-forward.sh
```

## ğŸ“Š UnterstÃ¼tzte Modelle und Anforderungen

| ModellgrÃ¶ÃŸe | GPU-Setup | Empfohlene Konfiguration | Beispielmodelle |
|-------------|-----------|--------------------------|-----------------|
| 2-3B | 1Ã— V100 | Standard (float16) | microsoft/phi-2, google/gemma-2b |
| 7B | 1Ã— V100 | AWQ Quantisierung | Mistral-7B-Instruct, Llama-2-7b-chat |
| 7B | 2Ã— V100 | Sharded Mode | Mistral-7B-Instruct, Llama-2-7b-chat |
| 13B | 2Ã— V100 | AWQ + Sharded | Llama-2-13b-chat |
| 13B | 4Ã— V100 | Sharded Mode | Llama-2-13b-chat |

## ğŸ”§ Wichtige Befehle

```bash
# Modellwechsel
./scripts/change-model.sh --model "mistralai/Mistral-7B-Instruct-v0.2" --quantization awq

# Skalierung auf mehrere GPUs
./scripts/scale-gpu.sh --count 2 --mem 16Gi

# Ãœberwachung
./scripts/monitor-gpu.sh            # GPU-Nutzung Ã¼berwachen
./scripts/check-logs.sh tgi -a      # Logs analysieren
./scripts/test-gpu.sh               # GPU-FunktionalitÃ¤t testen

# Fehlerbehebung
./scripts/test-v100-compatibility.sh  # V100-KompatibilitÃ¤t testen
./scripts/deploy-tgi-minimal.sh       # Minimales Testdeployment
```

Eine vollstÃ¤ndige Befehlsreferenz finden Sie in [COMMANDS.md](COMMANDS.md).

## ğŸ“ Projektstruktur

```
icc-Inference-deployment/
â”œâ”€â”€ configs/                # Konfigurationen
â”‚   â”œâ”€â”€ config.v100.sh      # V100-optimierte Konfiguration
â”‚   â””â”€â”€ config.example.sh   # Beispielkonfiguration
â”œâ”€â”€ scripts/                # Deployment- und Verwaltungsskripte
â”‚   â”œâ”€â”€ deploy-tgi-v100.sh  # TGI V100-Deployment
â”‚   â”œâ”€â”€ deploy-vllm-v100.sh # vLLM V100-Deployment
â”‚   â”œâ”€â”€ deploy-webui.sh     # WebUI-Deployment
â”‚   â”œâ”€â”€ port-forward.sh     # Port-Forwarding-Skript
â”‚   â””â”€â”€ ...                 # Weitere Hilfsskripte
â”œâ”€â”€ deploy-v100.sh          # Hauptdeployment-Skript (interaktiv)
â”œâ”€â”€ COMMANDS.md             # Befehlsreferenz
â”œâ”€â”€ V100-OPTIMIZATION.md    # V100-spezifische Optimierungen
â””â”€â”€ README.md               # Diese Dokumentation
```

## ğŸ› ï¸ Fehlerbehebung

Bei Problemen helfen folgende Schritte:

1. **Logs prÃ¼fen**: `./scripts/check-logs.sh tgi -a`
2. **GPU-Test**: `./scripts/test-gpu.sh`
3. **Minimaltest**: `./scripts/deploy-tgi-minimal.sh`
4. **Pod-Beschreibung**: `kubectl -n $NAMESPACE describe pod -l app=llm-server`

Typische Probleme und detaillierte LÃ¶sungen finden Sie in [COMMANDS.md](COMMANDS.md#fehlerbehebung).

## ğŸ“ Dokumentation

- [COMMANDS.md](COMMANDS.md) - VollstÃ¤ndige Befehlsreferenz mit Beispielen
- [V100-OPTIMIZATION.md](V100-OPTIMIZATION.md) - Detaillierte V100-Optimierungen
- [DOCUMENTATION.md](DOCUMENTATION.md) - AusfÃ¼hrliche technische Dokumentation

## ğŸ“„ Lizenz

Dieses Projekt steht unter der [MIT-Lizenz](LICENSE).

## ğŸ™ Danksagungen

- [Hugging Face Text Generation Inference](https://github.com/huggingface/text-generation-inference)
- [vLLM Project](https://github.com/vllm-project/vllm)
- [Open WebUI](https://github.com/open-webui/open-webui)
- HAW Hamburg Informatik Compute Cloud (ICC) Team
