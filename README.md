# IC-LLM: V100-optimiertes LLM-Deployment System

Eine umfassende LÃ¶sung fÃ¼r das Deployment von Large Language Models (LLMs) auf NVIDIA Tesla V100 GPUs in der HAW Hamburg Informatik Compute Cloud (ICC).

<div align="center">
  <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/TGI.png" alt="LLM Deployment" width="600px">
</div>

## ğŸŒŸ Hauptfeatures

- **V100-optimierte Konfiguration** fÃ¼r NVIDIA Tesla V100 GPUs
- **Zwei Inference-Engines**: Text Generation Inference (TGI) und vLLM
- **Multi-GPU-UnterstÃ¼tzung** mit bis zu 4x V100 GPUs im Sharded-Modus (TGI) oder Tensor-Parallel-Modus (vLLM)
- **OpenAI-kompatible REST API** fÃ¼r einfache Integration
- **Benutzerfreundliche WebUI** fÃ¼r Chat-Interaktionen
- **UnterstÃ¼tzung fÃ¼r zahlreiche Modelle**: Mistral, Llama, Gemma, Phi, etc.
- **Speicheroptimierungen**: AWQ/GPTQ Quantisierung, optimierte KontextlÃ¤ngen

## ğŸ“‹ Voraussetzungen

- HAW Hamburg infw-Account mit ICC-Zugang
- kubectl-Client auf Ihrem lokalen System
- VPN-Verbindung zum HAW-Netz (bei Remote-Zugriff)

## ğŸš€ Schnellstart

```bash
# Repository klonen
git clone https://github.com/scimbe/icc-Inference-deployment.git
cd icc-Inference-deployment

# Berechtigung setzen
chmod +x scripts/*.sh
chmod +x *.sh

# ICC-Login durchfÃ¼hren (einmalig)
./scripts/icc-login.sh

# Deployment starten (interaktiver Modus)
./deploy-v100.sh
```

Nach dem Deployment kÃ¶nnen Sie die WebUI unter http://localhost:3000 und die API unter http://localhost:8000 erreichen.

## ğŸ–¥ï¸ Manuelle Installation

```bash
# 1. V100-optimierte Konfiguration kopieren
cp configs/config.v100.sh configs/config.sh

# 2. Konfiguration anpassen (wichtig!)
#    - NAMESPACE auf Ihre w-Kennung + "-default" setzen
#    - Modell und GPU-Anzahl wÃ¤hlen
#    - ENGINE_TYPE auf "tgi" oder "vllm" setzen (je nach Bedarf)
nano configs/config.sh

# 3. TGI mit V100-Optimierungen deployen
./scripts/deploy-tgi-v100.sh

# 4. ODER: vLLM deployen (empfohlen fÃ¼r bestimmte AnwendungsfÃ¤lle)
./scripts/deploy-vllm-v100.sh

# 5. Web-OberflÃ¤che installieren
./scripts/deploy-webui.sh

# 6. Zugriff einrichten
./scripts/port-forward.sh
```

## ğŸ“Š TGI vs vLLM: Vergleich der Inference-Engines

| Feature | TGI | vLLM |
|---------|-----|------|
| Performance | Gut fÃ¼r 1-7B Modelle | Bessere Latenz, besonders bei >7B Modellen |
| Speichereffizienz | Standard-Inferenz | Optimiert durch PagedAttention |
| Multi-GPU | Sharded-Modus | Tensor-Parallel-Modus |
| Quantisierung | AWQ, GPTQ | AWQ, GPTQ, GGUF |
| Batchverarbeitung | Gut | Sehr gut (hÃ¶herer Durchsatz) |
| WebUI-Integration | VollstÃ¤ndig | VollstÃ¤ndig |
| V100-KompatibilitÃ¤t | Sehr gut | Gut (erfordert ggf. zusÃ¤tzliche Parameter) |

**Wann welche Engine verwenden?**
- **TGI**: Einfachere Konfiguration, sehr stabil, bessere UnterstÃ¼tzung fÃ¼r MoE-Modelle (Mixtral)
- **vLLM**: HÃ¶herer Durchsatz, geringere Latenz, besser fÃ¼r Anwendungen mit vielen gleichzeitigen Anfragen

## ğŸ“Š UnterstÃ¼tzte Modelle und Anforderungen

| ModellgrÃ¶ÃŸe | GPU-Setup | Empfohlene Konfiguration | Beispielmodelle |
|-------------|-----------|--------------------------|-----------------|
| 2-3B | 1Ã— V100 | Standard (float16) | microsoft/phi-2, google/gemma-2b |
| 7B | 1Ã— V100 | AWQ/GPTQ Quantisierung | TheBloke/Mistral-7B-Instruct-v0.2-GPTQ |
| 7B | 2Ã— V100 | Sharded/Tensor-Parallel | Mistral-7B-Instruct, Llama-2-7b-chat |
| 13B | 2Ã— V100 | AWQ/GPTQ + Sharded/TP | TheBloke/Llama-2-13b-chat-GPTQ |
| 13B | 4Ã— V100 | Sharded/Tensor-Parallel | Llama-2-13b-chat |

## ğŸ”§ Wichtige Befehle

```bash
# Modellwechsel
./scripts/change-model.sh --model "TheBloke/Mistral-7B-Instruct-v0.2-GPTQ" --quantization gptq

# Skalierung auf mehrere GPUs
./scripts/scale-gpu.sh --count 2 --mem 16Gi

# Ãœberwachung
./scripts/monitor-gpu.sh            # GPU-Nutzung Ã¼berwachen
./scripts/check-logs.sh tgi -a      # TGI-Logs analysieren
./scripts/check-logs.sh vllm -a     # vLLM-Logs analysieren
./scripts/test-gpu.sh               # GPU-FunktionalitÃ¤t testen

# Fehlerbehebung
./scripts/test-v100-compatibility.sh  # V100-KompatibilitÃ¤t testen
./scripts/deploy-tgi-minimal.sh       # Minimales TGI-Testdeployment
./scripts/deploy-vllm-minimal.sh      # Minimales vLLM-Testdeployment
```

Eine vollstÃ¤ndige Befehlsreferenz finden Sie in [COMMANDS.md](COMMANDS.md).

## ğŸ“ Projektstruktur

```
icc-Inference-deployment/
â”œâ”€â”€ configs/                # Konfigurationen
â”‚   â”œâ”€â”€ config.v100.sh      # V100-optimierte Konfiguration
â”‚   â””â”€â”€ config.example.sh   # Beispielkonfiguration
â”œâ”€â”€ scripts/                # Deployment- und Verwaltungsskripte
â”‚   â”œâ”€â”€ deploy-tgi-v100.sh  # TGI V100-Deployment
â”‚   â”œâ”€â”€ deploy-vllm-v100.sh # vLLM V100-Deployment
â”‚   â”œâ”€â”€ deploy-webui.sh     # WebUI-Deployment
â”‚   â”œâ”€â”€ port-forward.sh     # Port-Forwarding-Skript
â”‚   â””â”€â”€ ...                 # Weitere Hilfsskripte
â”œâ”€â”€ deploy-v100.sh          # Hauptdeployment-Skript (interaktiv)
â”œâ”€â”€ COMMANDS.md             # Befehlsreferenz
â”œâ”€â”€ TROUBLESHOOTING.md      # Fehlerbehebungsanleitung
â”œâ”€â”€ V100-OPTIMIZATION.md    # V100-spezifische Optimierungen
â””â”€â”€ README.md               # Diese Dokumentation
```

## ğŸ› ï¸ Fehlerbehebung

Bei Problemen helfen folgende Schritte:

1. **Logs prÃ¼fen**: 
   ```bash
   ./scripts/check-logs.sh tgi -a   # fÃ¼r TGI
   ./scripts/check-logs.sh vllm -a  # fÃ¼r vLLM
   ```

2. **GPU-Test durchfÃ¼hren**: 
   ```bash
   ./scripts/test-gpu.sh
   ```

3. **Minimaltests ausfÃ¼hren**:
   ```bash 
   ./scripts/deploy-tgi-minimal.sh   # fÃ¼r TGI
   ./scripts/deploy-vllm-minimal.sh  # fÃ¼r vLLM
   ```

4. **Pod-Beschreibung anzeigen**:
   ```bash
   kubectl -n $NAMESPACE describe pod -l app=llm-server      # fÃ¼r TGI
   kubectl -n $NAMESPACE describe pod -l service=vllm-server # fÃ¼r vLLM
   ```

Detaillierte Fehlerbehebungstipps finden Sie in [TROUBLESHOOTING.md](TROUBLESHOOTING.md).

## ğŸ” vLLM-spezifische Tipps

**Optimale Leistung mit vLLM:**

1. **Tensor Parallel Size anpassen**: 
   ```bash
   # In config.sh setzen:
   export TENSOR_PARALLEL_SIZE=2  # Bei Verwendung von 2 GPUs
   ```

2. **PagedAttention optimieren**:
   - Experimenten Sie mit verschiedenen Block-GrÃ¶ÃŸen fÃ¼r optimale Leistung
   ```bash
   # In config.sh anpassen:
   export BLOCK_SIZE=16  # MÃ¶gliche Werte: 8, 16, 32
   ```

3. **NCCL-Konfiguration** (fÃ¼r Multi-GPU):
   - Die Standard-NCCL-Konfiguration ist bereits fÃ¼r V100s optimiert
   - Bei Kommunikationsproblemen probieren Sie:
   ```bash
   export NCCL_P2P_DISABLE=1
   export NCCL_IB_DISABLE=1
   ```

4. **Erweiterte Quantisierungsoptionen**:
   - vLLM unterstÃ¼tzt mehrere Quantisierungsformate, GPTQ-Modelle zeigen gute Ergebnisse

## ğŸ“ Dokumentation

- [COMMANDS.md](COMMANDS.md) - VollstÃ¤ndige Befehlsreferenz mit Beispielen
- [TROUBLESHOOTING.md](TROUBLESHOOTING.md) - Detaillierte Fehlerbehebungsanleitung
- [V100-OPTIMIZATION.md](V100-OPTIMIZATION.md) - Detaillierte V100-Optimierungen
- [DOCUMENTATION.md](DOCUMENTATION.md) - AusfÃ¼hrliche technische Dokumentation

## ğŸ“„ Lizenz

Dieses Projekt steht unter der [MIT-Lizenz](LICENSE).

## ğŸ™ Danksagungen

- [Hugging Face Text Generation Inference](https://github.com/huggingface/text-generation-inference)
- [vLLM Project](https://github.com/vllm-project/vllm)
- [Open WebUI](https://github.com/open-webui/open-webui)
- HAW Hamburg Informatik Compute Cloud (ICC) Team
