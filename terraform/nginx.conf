worker_processes auto;
events {}

http {
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log warn;

    upstream tgi_backend {
        server host.docker.internal:8000;  # Der Name muss mit deinem Docker/Kubernetes-Netzwerk übereinstimmen
    }

    server {
        listen 8000;

        # Open WebUI API erwartet OpenAI-kompatible Endpunkte
        location /completions {
            proxy_pass http://tgi_backend/generate;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Content-Type "application/json";
        }

        # Streaming-Generierung
        location /chat/completions {
            proxy_pass http://tgi_backend/generate_stream;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header Content-Type "application/json";
            proxy_buffering off;  # Wichtig für Streams!
        }

        # Open WebUI Healthcheck → TGI
        location /health {
            proxy_pass http://tgi_backend/health;
        }

        # Open WebUI Model-Infos → TGI
        location /models {
            proxy_pass http://tgi_backend/info;
        }

        # Catch-all: Alle anderen OpenAI API-Calls direkt an TGI weiterleiten
        location / {
            proxy_pass http://tgi_backend;
        }
    }
}
