# ICC V100 LLM Deployment - Befehlsreferenz

Diese Referenz enth√§lt alle wichtigen Befehle zur Verwaltung Ihres LLM-Deployments (TGI oder vLLM) auf der ICC mit V100 GPU-Unterst√ºtzung.

## üìö Inhaltsverzeichnis

- [Schnellstart](#schnellstart)
- [Deployment-Befehle](#deployment-befehle)
- [Zugriff und Monitoring](#zugriff-und-monitoring)
- [Modell- und GPU-Verwaltung](#modell--und-gpu-verwaltung)
- [Fehlerbehebung](#fehlerbehebung)
- [Modellempfehlungen f√ºr V100 GPUs](#modellempfehlungen-f√ºr-v100-gpus)
- [Beispiele](#beispiele)

## üöÄ Schnellstart

```bash
# 1. Repository klonen und Verzeichnis wechseln
git clone https://github.com/scimbe/icc-llm-deployment.git
cd icc-llm-deployment

# 2. Skript-Berechtigungen setzen
chmod +x scripts/*.sh chmod +x *.sh

# 3. ICC-Zugang einrichten (einmalig)
./scripts/icc-login.sh

# 4. V100-optimierte Konfiguration erstellen
cp configs/config.v100.sh configs/config.sh

# 5. Konfiguration anpassen (wichtig!)
# - Namespace auf Ihre w-Kennung + "-default" setzen
# - Modell w√§hlen
nano configs/config.sh

# 6. Deployment starten (interaktiv)
./deploy-v100.sh

# 7. ODER: Direkt TGI mit V100-Optimierung starten
./scripts/deploy-tgi-v100.sh

# 8. WebUI installieren
./scripts/deploy-webui.sh

# 9. Zugriff einrichten
./scripts/port-forward.sh

# 10. Browser √∂ffnen
# http://localhost:3000 (WebUI)
# http://localhost:8000 (API)
```

## üõ†Ô∏è Deployment-Befehle

| Befehl | Beschreibung |
|--------|--------------|
| `./deploy-v100.sh` | Interaktives Deployment mit Men√ºf√ºhrung (empfohlen) |
| `./scripts/deploy-tgi-v100.sh` | TGI mit V100-Optimierungen deployen |
| `./scripts/deploy-vllm-v100.sh` | vLLM mit V100-Optimierungen deployen |
| `./scripts/deploy-webui.sh` | Web-Benutzeroberfl√§che installieren |
| `./scripts/cleanup.sh` | Alle erstellten Ressourcen entfernen |

### Konfiguration

```bash
# IC-Login (bei erstem Zugriff)
./scripts/icc-login.sh

# Konfiguration anpassen
cp configs/config.v100.sh configs/config.sh
nano configs/config.sh
```

## üîå Zugriff und Monitoring

| Befehl | Beschreibung |
|--------|--------------|
| `./scripts/port-forward.sh` | Port-Forwarding f√ºr API und WebUI einrichten |
| `./scripts/port-forward.sh --api-port=9000` | Benutzerdefinierten API-Port verwenden |
| `./scripts/monitor-gpu.sh` | GPU-Auslastung in Echtzeit √ºberwachen |
| `./scripts/monitor-gpu.sh -i 5 -f full` | Detailliertes GPU-Monitoring mit 5s-Intervall |
| `./scripts/check-logs.sh tgi` | Logs des TGI-Servers anzeigen |
| `./scripts/check-logs.sh webui` | Logs der WebUI anzeigen |
| `./scripts/check-logs.sh tgi -a` | TGI-Logs analysieren und Probleme identifizieren |
| `./scripts/test-gpu.sh` | GPU-Funktionalit√§t testen |

## üîÑ Modell- und GPU-Verwaltung

### Modellwechsel

```bash
# Modell wechseln (Standard-Pr√§zision)
./scripts/change-model.sh --model "microsoft/phi-2"

# Modell mit Quantisierung (f√ºr gr√∂√üere Modelle auf einer GPU)
./scripts/change-model.sh --model "mistralai/Mistral-7B-Instruct-v0.2" --quantization awq

# Modell mit angepassten Parametern
./scripts/change-model.sh --model "NousResearch/Hermes-3-Llama-3.1-8B" --max-len 2048 --temp 0.7
```

### GPU-Skalierung

```bash
# Auf 2 GPUs skalieren (f√ºr gr√∂√üere Modelle)
./scripts/scale-gpu.sh --count 2

# Auf 1 GPU zur√ºcksetzen mit angepasstem Shared Memory
./scripts/scale-gpu.sh --count 1 --mem 8Gi

# Multi-GPU mit mehr Shared Memory f√ºr 13B+ Modelle
./scripts/scale-gpu.sh --count 4 --mem 16Gi
```

## üîç Fehlerbehebung

| Befehl | Beschreibung |
|--------|--------------|
| `./scripts/test-v100-compatibility.sh` | V100-GPU-Kompatibilit√§t √ºberpr√ºfen |
| `./scripts/deploy-tgi-minimal.sh` | Minimales Test-Deployment mit TinyLlama starten |
| `kubectl -n $NAMESPACE describe pod -l app=llm-server` | Pod-Details f√ºr Fehlerdiagnose anzeigen |
| `kubectl -n $NAMESPACE get events` | Kubernetes-Events f√ºr Fehlermeldungen anzeigen |
| `./scripts/reset-and-test-tgi.sh` | TGI zur√ºcksetzen und mit Minimal-Setup testen |

### H√§ufige Probleme und L√∂sungen

#### Out-of-Memory (OOM) Fehler

**Symptom:**
```
Shard process was signaled to shutdown with signal 9
```

**L√∂sungen:**
- Kleineres Modell: `./scripts/change-model.sh --model "microsoft/phi-2"`
- Quantisierung aktivieren: `./scripts/change-model.sh --model "mistralai/Mistral-7B-Instruct-v0.2" --quantization awq`
- Mehr GPUs: `./scripts/scale-gpu.sh --count 2`
- Kontextl√§nge reduzieren: In `config.sh` setzen: `export MAX_INPUT_LENGTH=1024` und `export MAX_TOTAL_TOKENS=2048`

#### Modell l√§dt nicht

**L√∂sungen:**
- Logs √ºberpr√ºfen: `./scripts/check-logs.sh tgi`
- Hugging Face Token in `config.sh` setzen
- TGI-Version √ºberpr√ºfen: `kubectl -n $NAMESPACE describe pod -l app=llm-server | grep Image`
- Minimales Deployment testen: `./scripts/deploy-tgi-minimal.sh`

## üìä Modellempfehlungen f√ºr V100 GPUs

| Modellgr√∂√üe | GPU-Setup | Quantisierung | Empfohlene Modelle |
|-------------|-----------|---------------|-------------------|
| 1-3B | 1√ó V100 (16GB) | Keine | microsoft/phi-2, google/gemma-2b |
| 7B | 1√ó V100 (16GB) | AWQ | Mistral-7B, Llama-2-7b |
| 7B | 2√ó V100 (32GB) | Keine | Mistral-7B, Llama-2-7b |
| 13B | 2√ó V100 (32GB) | AWQ | Llama-2-13b |
| 13B | 4√ó V100 (64GB) | Keine | Llama-2-13b |
| 34B+ | Nicht empfohlen | - | F√ºr gr√∂√üere Modelle A100 GPUs verwenden |

## üìù Beispiele

### Komplettes Setup mit TGI f√ºr Mistral-7B mit WebUI

```bash
# 1. Konfiguration anpassen
cp configs/config.v100.sh configs/config.sh
sed -i 's/wXYZ123-default/wABC123-default/' configs/config.sh  # Namespace anpassen
sed -i 's/MODEL_NAME=".*"/MODEL_NAME="mistralai\/Mistral-7B-Instruct-v0.2"/' configs/config.sh

# 2. TGI mit AWQ-Quantisierung deployen (f√ºr 1 GPU)
export QUANTIZATION="awq"
./scripts/deploy-tgi-v100.sh

# 3. WebUI installieren
./scripts/deploy-webui.sh

# 4. Zugriff einrichten
./scripts/port-forward.sh
```

### Multi-GPU Setup f√ºr Llama-2-13B

```bash
# 1. Konfiguration anpassen
cp configs/config.v100.sh configs/config.sh
# Namespace und Token anpassen
sed -i 's/wXYZ123-default/wABC123-default/' configs/config.sh
sed -i 's/HUGGINGFACE_TOKEN=""/HUGGINGFACE_TOKEN="hf_..."/' configs/config.sh
sed -i 's/MODEL_NAME=".*"/MODEL_NAME="meta-llama\/Llama-2-13b-chat-hf"/' configs/config.sh

# 2. Auf 2 GPUs skalieren
sed -i 's/GPU_COUNT=1/GPU_COUNT=2/' configs/config.sh

# 3. Shared Memory erh√∂hen
sed -i 's/DSHM_SIZE="8Gi"/DSHM_SIZE="16Gi"/' configs/config.sh

# 4. TGI Server deployen
./scripts/deploy-tgi-v100.sh

# 5. WebUI installieren
./scripts/deploy-webui.sh

# 6. Zugriff einrichten
./scripts/port-forward.sh
```

### vLLM Deployment mit TinyLlama f√ºr Tests

```bash
# 1. Konfiguration anpassen
cp configs/config.v100.sh configs/config.sh
sed -i 's/wXYZ123-default/wABC123-default/' configs/config.sh
sed -i 's/ENGINE_TYPE="tgi"/ENGINE_TYPE="vllm"/' configs/config.sh
sed -i 's/MODEL_NAME=".*"/MODEL_NAME="TinyLlama\/TinyLlama-1.1B-Chat-v1.0"/' configs/config.sh

# 2. vLLM deployen
./scripts/deploy-vllm-v100.sh

# 3. WebUI installieren
./scripts/deploy-webui.sh

# 4. Zugriff einrichten
./scripts/port-forward.sh
```

## üìà Performance-Tipps

1. **Verwenden Sie AWQ-Quantisierung** f√ºr Modelle ‚â•7B auf einer einzelnen GPU
2. **Sharded Mode** (Multi-GPU) f√ºr Modelle ‚â•13B ohne Quantisierung
3. **Reduzieren Sie die Kontextl√§nge** bei Speicherproblemen
4. **Passen Sie CUDA_MEMORY_FRACTION** (0.8 bis 0.9) nach Bedarf an
5. **Verwenden Sie mehr dshm** (Shared Memory) bei Multi-GPU
6. **vLLM** kann bei einigen Modellen besser performen als TGI
7. **Testen Sie verschiedene BLOCK_SIZE-Werte** (8, 16, 32) in vLLM f√ºr optimale Performance

## üîñ Weitere Informationen

- [V100-OPTIMIZATION.md](V100-OPTIMIZATION.md) - Detaillierte V100-spezifische Optimierungen
- [DOCUMENTATION.md](DOCUMENTATION.md) - Vollst√§ndige Projektdokumentation
- [Text Generation Inference](https://github.com/huggingface/text-generation-inference) - TGI-Dokumentation
- [vLLM](https://github.com/vllm-project/vllm) - vLLM-Dokumentation
